{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-3: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\_YiyuanData\\大四\\课程\\深度学习导论\\homework-3\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.4183\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.4111\t Accuracy 0.2284\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2269\t Accuracy 0.3341\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1632\t Accuracy 0.4068\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1304\t Accuracy 0.4614\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1103\t Accuracy 0.5075\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0967\t Accuracy 0.5431\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0868\t Accuracy 0.5730\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0793\t Accuracy 0.5987\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0732\t Accuracy 0.6194\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0683\t Accuracy 0.6383\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0643\t Average training accuracy 0.6540\n",
      "Epoch [0]\t Average validation loss 0.0213\t Average validation accuracy 0.8476\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0205\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0221\t Accuracy 0.8225\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0219\t Accuracy 0.8290\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0220\t Accuracy 0.8271\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0218\t Accuracy 0.8291\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0216\t Accuracy 0.8311\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0214\t Accuracy 0.8335\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0213\t Accuracy 0.8353\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0211\t Accuracy 0.8375\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0209\t Accuracy 0.8392\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0208\t Accuracy 0.8413\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0206\t Average training accuracy 0.8431\n",
      "Epoch [1]\t Average validation loss 0.0167\t Average validation accuracy 0.8950\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0161\t Accuracy 0.9100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0178\t Accuracy 0.8724\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0178\t Accuracy 0.8700\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0180\t Accuracy 0.8654\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0180\t Accuracy 0.8666\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0178\t Accuracy 0.8680\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0177\t Accuracy 0.8690\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0177\t Accuracy 0.8688\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0176\t Accuracy 0.8700\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0175\t Accuracy 0.8706\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0174\t Accuracy 0.8712\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0173\t Average training accuracy 0.8718\n",
      "Epoch [2]\t Average validation loss 0.0142\t Average validation accuracy 0.9106\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0137\t Accuracy 0.9200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0153\t Accuracy 0.8949\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0154\t Accuracy 0.8877\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0157\t Accuracy 0.8824\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0156\t Accuracy 0.8832\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0155\t Accuracy 0.8839\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0155\t Accuracy 0.8844\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0155\t Accuracy 0.8836\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0154\t Accuracy 0.8843\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0154\t Accuracy 0.8846\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0154\t Accuracy 0.8847\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0153\t Average training accuracy 0.8851\n",
      "Epoch [3]\t Average validation loss 0.0126\t Average validation accuracy 0.9192\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0121\t Accuracy 0.9200\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0137\t Accuracy 0.9035\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0139\t Accuracy 0.8968\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0141\t Accuracy 0.8926\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0141\t Accuracy 0.8943\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0140\t Accuracy 0.8948\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0140\t Accuracy 0.8947\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0140\t Accuracy 0.8936\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0140\t Accuracy 0.8940\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0140\t Accuracy 0.8942\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0140\t Accuracy 0.8941\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0139\t Average training accuracy 0.8944\n",
      "Epoch [4]\t Average validation loss 0.0115\t Average validation accuracy 0.9246\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0110\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0125\t Accuracy 0.9112\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0128\t Accuracy 0.9046\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0131\t Accuracy 0.9001\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0130\t Accuracy 0.9012\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0130\t Accuracy 0.9017\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0129\t Accuracy 0.9012\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0130\t Accuracy 0.9002\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0130\t Accuracy 0.9004\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0130\t Accuracy 0.9005\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0130\t Accuracy 0.8999\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0129\t Average training accuracy 0.9003\n",
      "Epoch [5]\t Average validation loss 0.0108\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0103\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0117\t Accuracy 0.9151\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0120\t Accuracy 0.9089\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0123\t Accuracy 0.9044\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0122\t Accuracy 0.9059\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0122\t Accuracy 0.9067\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0122\t Accuracy 0.9066\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0122\t Accuracy 0.9054\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0122\t Accuracy 0.9053\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0122\t Accuracy 0.9051\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0122\t Accuracy 0.9044\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0122\t Average training accuracy 0.9046\n",
      "Epoch [6]\t Average validation loss 0.0102\t Average validation accuracy 0.9286\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0098\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0111\t Accuracy 0.9171\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0114\t Accuracy 0.9118\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0117\t Accuracy 0.9078\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0116\t Accuracy 0.9090\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0116\t Accuracy 0.9098\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0116\t Accuracy 0.9099\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0116\t Accuracy 0.9087\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0116\t Accuracy 0.9085\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0117\t Accuracy 0.9084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0117\t Accuracy 0.9078\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0116\t Average training accuracy 0.9081\n",
      "Epoch [7]\t Average validation loss 0.0098\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0094\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0107\t Accuracy 0.9196\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0109\t Accuracy 0.9145\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0112\t Accuracy 0.9111\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0112\t Accuracy 0.9123\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0112\t Accuracy 0.9129\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0112\t Accuracy 0.9132\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0112\t Accuracy 0.9123\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0112\t Accuracy 0.9120\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0112\t Accuracy 0.9116\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0113\t Accuracy 0.9110\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0112\t Average training accuracy 0.9110\n",
      "Epoch [8]\t Average validation loss 0.0095\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0091\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0103\t Accuracy 0.9208\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0106\t Accuracy 0.9164\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0109\t Accuracy 0.9132\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0108\t Accuracy 0.9143\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0108\t Accuracy 0.9151\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0108\t Accuracy 0.9154\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0109\t Accuracy 0.9144\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0109\t Accuracy 0.9142\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0109\t Accuracy 0.9138\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0109\t Accuracy 0.9131\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0109\t Average training accuracy 0.9131\n",
      "Epoch [9]\t Average validation loss 0.0092\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0089\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0101\t Accuracy 0.9210\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0103\t Accuracy 0.9176\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0106\t Accuracy 0.9152\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0105\t Accuracy 0.9162\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0105\t Accuracy 0.9167\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0105\t Accuracy 0.9170\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0106\t Accuracy 0.9161\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0106\t Accuracy 0.9158\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0106\t Accuracy 0.9154\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0107\t Accuracy 0.9148\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0106\t Average training accuracy 0.9147\n",
      "Epoch [10]\t Average validation loss 0.0090\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0087\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0098\t Accuracy 0.9229\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0101\t Accuracy 0.9196\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0103\t Accuracy 0.9169\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0103\t Accuracy 0.9181\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0103\t Accuracy 0.9184\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0103\t Accuracy 0.9185\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0104\t Accuracy 0.9176\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0104\t Accuracy 0.9173\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0104\t Accuracy 0.9171\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0105\t Accuracy 0.9163\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0104\t Average training accuracy 0.9163\n",
      "Epoch [11]\t Average validation loss 0.0088\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0085\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0096\t Accuracy 0.9243\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0099\t Accuracy 0.9208\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0102\t Accuracy 0.9179\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0101\t Accuracy 0.9190\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0101\t Accuracy 0.9192\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0101\t Accuracy 0.9191\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0102\t Accuracy 0.9182\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0102\t Accuracy 0.9181\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0102\t Accuracy 0.9179\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0103\t Accuracy 0.9171\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0102\t Average training accuracy 0.9171\n",
      "Epoch [12]\t Average validation loss 0.0087\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0084\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0095\t Accuracy 0.9267\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0097\t Accuracy 0.9220\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0100\t Accuracy 0.9189\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0099\t Accuracy 0.9199\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0100\t Accuracy 0.9200\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0100\t Accuracy 0.9200\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0100\t Accuracy 0.9191\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0100\t Accuracy 0.9190\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0101\t Accuracy 0.9189\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0101\t Accuracy 0.9182\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0101\t Average training accuracy 0.9181\n",
      "Epoch [13]\t Average validation loss 0.0085\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0083\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0094\t Accuracy 0.9276\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0096\t Accuracy 0.9233\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0099\t Accuracy 0.9201\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0098\t Accuracy 0.9208\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0098\t Accuracy 0.9209\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0098\t Accuracy 0.9208\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0099\t Accuracy 0.9199\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0099\t Accuracy 0.9197\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0099\t Accuracy 0.9197\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0100\t Accuracy 0.9189\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0100\t Average training accuracy 0.9189\n",
      "Epoch [14]\t Average validation loss 0.0084\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0082\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0092\t Accuracy 0.9284\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0094\t Accuracy 0.9243\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0097\t Accuracy 0.9210\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0097\t Accuracy 0.9216\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0097\t Accuracy 0.9216\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0097\t Accuracy 0.9214\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0098\t Accuracy 0.9204\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0098\t Accuracy 0.9203\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0098\t Accuracy 0.9203\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0099\t Accuracy 0.9196\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0098\t Average training accuracy 0.9195\n",
      "Epoch [15]\t Average validation loss 0.0083\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0081\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0091\t Accuracy 0.9292\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0093\t Accuracy 0.9252\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0096\t Accuracy 0.9218\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0096\t Accuracy 0.9224\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0096\t Accuracy 0.9225\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0096\t Accuracy 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0097\t Accuracy 0.9212\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0097\t Accuracy 0.9211\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0097\t Accuracy 0.9211\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0098\t Accuracy 0.9203\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0097\t Average training accuracy 0.9203\n",
      "Epoch [16]\t Average validation loss 0.0083\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0080\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0090\t Accuracy 0.9298\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0092\t Accuracy 0.9258\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0095\t Accuracy 0.9226\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0095\t Accuracy 0.9232\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0095\t Accuracy 0.9234\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0095\t Accuracy 0.9231\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0096\t Accuracy 0.9221\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0096\t Accuracy 0.9221\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0096\t Accuracy 0.9220\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0097\t Accuracy 0.9212\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0097\t Average training accuracy 0.9211\n",
      "Epoch [17]\t Average validation loss 0.0082\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0080\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0090\t Accuracy 0.9300\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0092\t Accuracy 0.9261\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0094\t Accuracy 0.9229\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0094\t Accuracy 0.9236\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0094\t Accuracy 0.9239\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0094\t Accuracy 0.9236\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0095\t Accuracy 0.9226\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0095\t Accuracy 0.9225\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0095\t Accuracy 0.9225\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0096\t Accuracy 0.9219\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0096\t Average training accuracy 0.9219\n",
      "Epoch [18]\t Average validation loss 0.0081\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0079\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0089\t Accuracy 0.9308\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0091\t Accuracy 0.9271\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0094\t Accuracy 0.9239\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0093\t Accuracy 0.9245\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0093\t Accuracy 0.9247\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0094\t Accuracy 0.9243\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0094\t Accuracy 0.9234\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0094\t Accuracy 0.9233\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0095\t Accuracy 0.9233\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0095\t Accuracy 0.9226\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0095\t Average training accuracy 0.9225\n",
      "Epoch [19]\t Average validation loss 0.0081\t Average validation accuracy 0.9418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9291.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.4057\t Accuracy 0.1200\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0764\t Accuracy 0.5049\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0520\t Accuracy 0.6171\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0427\t Accuracy 0.6736\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0371\t Accuracy 0.7124\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0334\t Accuracy 0.7403\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0307\t Accuracy 0.7601\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0287\t Accuracy 0.7742\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0270\t Accuracy 0.7872\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0256\t Accuracy 0.7979\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0245\t Accuracy 0.8061\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0235\t Average training accuracy 0.8140\n",
      "Epoch [0]\t Average validation loss 0.0116\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0111\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0124\t Accuracy 0.9047\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0124\t Accuracy 0.9029\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0125\t Accuracy 0.9017\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0123\t Accuracy 0.9028\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0122\t Accuracy 0.9035\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0121\t Accuracy 0.9041\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0120\t Accuracy 0.9042\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0119\t Accuracy 0.9052\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0118\t Accuracy 0.9058\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0118\t Accuracy 0.9057\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0117\t Average training accuracy 0.9064\n",
      "Epoch [1]\t Average validation loss 0.0093\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0086\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0100\t Accuracy 0.9222\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0101\t Accuracy 0.9210\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0103\t Accuracy 0.9195\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0102\t Accuracy 0.9201\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0101\t Accuracy 0.9202\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0101\t Accuracy 0.9200\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0101\t Accuracy 0.9196\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0101\t Accuracy 0.9203\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0101\t Accuracy 0.9209\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0101\t Accuracy 0.9204\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0100\t Average training accuracy 0.9204\n",
      "Epoch [2]\t Average validation loss 0.0084\t Average validation accuracy 0.9462\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0078\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0089\t Accuracy 0.9308\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0091\t Accuracy 0.9285\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0093\t Accuracy 0.9272\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0092\t Accuracy 0.9278\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0092\t Accuracy 0.9282\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0092\t Accuracy 0.9282\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0092\t Accuracy 0.9277\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0092\t Accuracy 0.9283\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0092\t Accuracy 0.9289\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0092\t Accuracy 0.9286\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0091\t Average training accuracy 0.9285\n",
      "Epoch [3]\t Average validation loss 0.0079\t Average validation accuracy 0.9504\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0073\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0083\t Accuracy 0.9375\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0084\t Accuracy 0.9349\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0086\t Accuracy 0.9334\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0086\t Accuracy 0.9341\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0086\t Accuracy 0.9348\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0086\t Accuracy 0.9347\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0086\t Accuracy 0.9340\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0086\t Accuracy 0.9346\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0086\t Accuracy 0.9353\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0086\t Accuracy 0.9347\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0086\t Average training accuracy 0.9345\n",
      "Epoch [4]\t Average validation loss 0.0075\t Average validation accuracy 0.9546\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0078\t Accuracy 0.9433\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0080\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0082\t Accuracy 0.9384\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0081\t Accuracy 0.9389\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0081\t Accuracy 0.9396\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0081\t Accuracy 0.9394\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0081\t Accuracy 0.9387\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0081\t Accuracy 0.9394\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0081\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0081\t Accuracy 0.9394\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0081\t Average training accuracy 0.9392\n",
      "Epoch [5]\t Average validation loss 0.0072\t Average validation accuracy 0.9568\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0074\t Accuracy 0.9473\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0076\t Accuracy 0.9445\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0078\t Accuracy 0.9432\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0077\t Accuracy 0.9436\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0077\t Accuracy 0.9439\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0077\t Accuracy 0.9434\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0077\t Accuracy 0.9428\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0077\t Accuracy 0.9434\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0077\t Accuracy 0.9440\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0078\t Accuracy 0.9433\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0077\t Average training accuracy 0.9431\n",
      "Epoch [6]\t Average validation loss 0.0069\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0064\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0071\t Accuracy 0.9508\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0073\t Accuracy 0.9486\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0075\t Accuracy 0.9472\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0074\t Accuracy 0.9475\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0074\t Accuracy 0.9478\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9473\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0074\t Accuracy 0.9467\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0074\t Accuracy 0.9469\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9473\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0075\t Accuracy 0.9465\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0074\t Average training accuracy 0.9464\n",
      "Epoch [7]\t Average validation loss 0.0067\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0063\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9527\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9510\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9497\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9504\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9498\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9493\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9495\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9499\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0072\t Average training accuracy 0.9492\n",
      "Epoch [8]\t Average validation loss 0.0065\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0062\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0066\t Accuracy 0.9557\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0068\t Accuracy 0.9536\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0070\t Accuracy 0.9524\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0069\t Accuracy 0.9526\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0069\t Accuracy 0.9528\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0069\t Accuracy 0.9524\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0069\t Accuracy 0.9521\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0069\t Accuracy 0.9522\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0069\t Accuracy 0.9526\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0070\t Accuracy 0.9518\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0070\t Average training accuracy 0.9518\n",
      "Epoch [9]\t Average validation loss 0.0064\t Average validation accuracy 0.9634\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0061\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0064\t Accuracy 0.9590\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0066\t Accuracy 0.9558\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0067\t Accuracy 0.9552\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0067\t Accuracy 0.9550\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0067\t Accuracy 0.9551\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0067\t Accuracy 0.9546\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0067\t Accuracy 0.9544\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0067\t Accuracy 0.9546\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0067\t Accuracy 0.9550\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0068\t Accuracy 0.9543\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0068\t Average training accuracy 0.9543\n",
      "Epoch [10]\t Average validation loss 0.0062\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0060\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0062\t Accuracy 0.9610\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0064\t Accuracy 0.9577\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0066\t Accuracy 0.9565\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0065\t Accuracy 0.9564\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0065\t Accuracy 0.9565\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0065\t Accuracy 0.9561\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0065\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0066\t Accuracy 0.9564\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0066\t Accuracy 0.9568\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0066\t Accuracy 0.9560\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0066\t Average training accuracy 0.9559\n",
      "Epoch [11]\t Average validation loss 0.0061\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0059\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0060\t Accuracy 0.9620\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0062\t Accuracy 0.9597\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0064\t Accuracy 0.9581\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0064\t Accuracy 0.9580\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0063\t Accuracy 0.9583\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0064\t Accuracy 0.9579\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0064\t Accuracy 0.9580\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0064\t Accuracy 0.9582\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0064\t Accuracy 0.9586\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0064\t Accuracy 0.9578\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0064\t Average training accuracy 0.9578\n",
      "Epoch [12]\t Average validation loss 0.0060\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0058\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0059\t Accuracy 0.9629\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0061\t Accuracy 0.9610\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0062\t Accuracy 0.9594\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0062\t Accuracy 0.9597\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0062\t Accuracy 0.9601\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0062\t Accuracy 0.9596\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0062\t Accuracy 0.9597\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0062\t Accuracy 0.9598\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0062\t Accuracy 0.9601\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0063\t Accuracy 0.9594\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0063\t Average training accuracy 0.9594\n",
      "Epoch [13]\t Average validation loss 0.0059\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0057\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0058\t Accuracy 0.9633\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0059\t Accuracy 0.9621\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0061\t Accuracy 0.9609\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0061\t Accuracy 0.9614\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0060\t Accuracy 0.9618\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0061\t Accuracy 0.9612\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0061\t Accuracy 0.9613\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0061\t Accuracy 0.9614\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0061\t Accuracy 0.9616\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0061\t Accuracy 0.9609\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0061\t Average training accuracy 0.9610\n",
      "Epoch [14]\t Average validation loss 0.0058\t Average validation accuracy 0.9680\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0056\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0056\t Accuracy 0.9651\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0058\t Accuracy 0.9640\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0060\t Accuracy 0.9628\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0059\t Accuracy 0.9630\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0059\t Accuracy 0.9633\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0060\t Accuracy 0.9626\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0060\t Accuracy 0.9627\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0060\t Accuracy 0.9627\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0060\t Accuracy 0.9629\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0060\t Accuracy 0.9622\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0060\t Average training accuracy 0.9621\n",
      "Epoch [15]\t Average validation loss 0.0057\t Average validation accuracy 0.9680\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0055\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0055\t Accuracy 0.9663\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0057\t Accuracy 0.9648\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0059\t Accuracy 0.9638\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0058\t Accuracy 0.9641\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0058\t Accuracy 0.9643\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0059\t Accuracy 0.9636\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0059\t Accuracy 0.9638\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0059\t Accuracy 0.9638\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0059\t Accuracy 0.9639\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0059\t Accuracy 0.9632\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0059\t Average training accuracy 0.9632\n",
      "Epoch [16]\t Average validation loss 0.0056\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0054\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0054\t Accuracy 0.9671\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0056\t Accuracy 0.9651\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0057\t Accuracy 0.9642\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0057\t Accuracy 0.9648\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0057\t Accuracy 0.9650\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0058\t Accuracy 0.9643\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0058\t Accuracy 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0058\t Accuracy 0.9647\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0058\t Accuracy 0.9648\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0058\t Accuracy 0.9642\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0058\t Average training accuracy 0.9641\n",
      "Epoch [17]\t Average validation loss 0.0055\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0054\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0054\t Accuracy 0.9680\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0055\t Accuracy 0.9667\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0057\t Accuracy 0.9658\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0056\t Accuracy 0.9659\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0056\t Accuracy 0.9661\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0057\t Accuracy 0.9652\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0057\t Accuracy 0.9656\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0057\t Accuracy 0.9655\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0057\t Accuracy 0.9656\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0057\t Accuracy 0.9649\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0057\t Average training accuracy 0.9649\n",
      "Epoch [18]\t Average validation loss 0.0055\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0053\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0053\t Accuracy 0.9690\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0055\t Accuracy 0.9675\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0056\t Accuracy 0.9666\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0056\t Accuracy 0.9668\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0055\t Accuracy 0.9670\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0056\t Accuracy 0.9662\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0056\t Accuracy 0.9664\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0056\t Accuracy 0.9663\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0056\t Accuracy 0.9663\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0056\t Accuracy 0.9656\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0056\t Average training accuracy 0.9657\n",
      "Epoch [19]\t Average validation loss 0.0054\t Average validation accuracy 0.9696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9634.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2318\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2325\t Accuracy 0.2800\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2253\t Accuracy 0.3846\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.2210\t Accuracy 0.4544\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.2172\t Accuracy 0.5144\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.2142\t Accuracy 0.5614\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.2115\t Accuracy 0.5976\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2094\t Accuracy 0.6238\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2076\t Accuracy 0.6468\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2060\t Accuracy 0.6651\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2045\t Accuracy 0.6805\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2031\t Average training accuracy 0.6938\n",
      "Epoch [0]\t Average validation loss 0.1861\t Average validation accuracy 0.8776\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1864\t Accuracy 0.9000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1874\t Accuracy 0.8420\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1872\t Accuracy 0.8420\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1871\t Accuracy 0.8371\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1867\t Accuracy 0.8394\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1863\t Accuracy 0.8407\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1858\t Accuracy 0.8430\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1856\t Accuracy 0.8435\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1853\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1850\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1847\t Accuracy 0.8483\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1844\t Average training accuracy 0.8499\n",
      "Epoch [1]\t Average validation loss 0.1779\t Average validation accuracy 0.9010\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1781\t Accuracy 0.9200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1798\t Accuracy 0.8786\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1799\t Accuracy 0.8721\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1803\t Accuracy 0.8660\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1802\t Accuracy 0.8679\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1800\t Accuracy 0.8682\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1798\t Accuracy 0.8691\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1798\t Accuracy 0.8683\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1797\t Accuracy 0.8698\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1796\t Accuracy 0.8701\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1794\t Accuracy 0.8703\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1792\t Average training accuracy 0.8714\n",
      "Epoch [2]\t Average validation loss 0.1741\t Average validation accuracy 0.9132\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1744\t Accuracy 0.9300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1761\t Accuracy 0.8929\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1763\t Accuracy 0.8872\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1767\t Accuracy 0.8817\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1767\t Accuracy 0.8833\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1765\t Accuracy 0.8842\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1764\t Accuracy 0.8846\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1765\t Accuracy 0.8834\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1764\t Accuracy 0.8842\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1764\t Accuracy 0.8843\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1763\t Accuracy 0.8842\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1761\t Average training accuracy 0.8850\n",
      "Epoch [3]\t Average validation loss 0.1716\t Average validation accuracy 0.9192\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1719\t Accuracy 0.9300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1735\t Accuracy 0.9025\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1738\t Accuracy 0.8970\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1743\t Accuracy 0.8921\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1743\t Accuracy 0.8935\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1741\t Accuracy 0.8944\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1741\t Accuracy 0.8951\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1742\t Accuracy 0.8940\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1742\t Accuracy 0.8945\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1741\t Accuracy 0.8946\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1741\t Accuracy 0.8946\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1740\t Average training accuracy 0.8951\n",
      "Epoch [4]\t Average validation loss 0.1698\t Average validation accuracy 0.9268\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1701\t Accuracy 0.9300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1717\t Accuracy 0.9110\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1720\t Accuracy 0.9047\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1725\t Accuracy 0.9007\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1725\t Accuracy 0.9020\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1724\t Accuracy 0.9026\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1723\t Accuracy 0.9027\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1725\t Accuracy 0.9018\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1725\t Accuracy 0.9019\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1725\t Accuracy 0.9022\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1725\t Accuracy 0.9019\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1724\t Average training accuracy 0.9022\n",
      "Epoch [5]\t Average validation loss 0.1684\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1687\t Accuracy 0.9300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1703\t Accuracy 0.9157\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1706\t Accuracy 0.9102\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1711\t Accuracy 0.9064\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1711\t Accuracy 0.9079\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1710\t Accuracy 0.9080\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1710\t Accuracy 0.9082\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1712\t Accuracy 0.9074\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1712\t Accuracy 0.9072\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1712\t Accuracy 0.9075\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1712\t Accuracy 0.9069\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1711\t Average training accuracy 0.9072\n",
      "Epoch [6]\t Average validation loss 0.1674\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1676\t Accuracy 0.9400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1692\t Accuracy 0.9192\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1695\t Accuracy 0.9137\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1700\t Accuracy 0.9099\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1700\t Accuracy 0.9118\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1699\t Accuracy 0.9122\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1700\t Accuracy 0.9122\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1702\t Accuracy 0.9115\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1701\t Accuracy 0.9111\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1702\t Accuracy 0.9114\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1702\t Accuracy 0.9107\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1701\t Average training accuracy 0.9109\n",
      "Epoch [7]\t Average validation loss 0.1665\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1668\t Accuracy 0.9400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1683\t Accuracy 0.9212\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1686\t Accuracy 0.9165\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1691\t Accuracy 0.9129\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1692\t Accuracy 0.9146\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1691\t Accuracy 0.9150\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1691\t Accuracy 0.9152\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1693\t Accuracy 0.9143\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1693\t Accuracy 0.9142\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1693\t Accuracy 0.9143\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1694\t Accuracy 0.9134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1693\t Average training accuracy 0.9135\n",
      "Epoch [8]\t Average validation loss 0.1658\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1661\t Accuracy 0.9400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1676\t Accuracy 0.9233\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1679\t Accuracy 0.9194\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1684\t Accuracy 0.9160\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1685\t Accuracy 0.9175\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1684\t Accuracy 0.9178\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1684\t Accuracy 0.9179\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1686\t Accuracy 0.9170\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1686\t Accuracy 0.9168\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1687\t Accuracy 0.9169\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1687\t Accuracy 0.9160\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1687\t Average training accuracy 0.9161\n",
      "Epoch [9]\t Average validation loss 0.1652\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1655\t Accuracy 0.9400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1670\t Accuracy 0.9253\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1673\t Accuracy 0.9212\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1678\t Accuracy 0.9175\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1679\t Accuracy 0.9193\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1678\t Accuracy 0.9193\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1679\t Accuracy 0.9193\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1681\t Accuracy 0.9184\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1681\t Accuracy 0.9183\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1681\t Accuracy 0.9186\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1682\t Accuracy 0.9177\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1681\t Average training accuracy 0.9178\n",
      "Epoch [10]\t Average validation loss 0.1648\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1650\t Accuracy 0.9400\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1666\t Accuracy 0.9271\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1668\t Accuracy 0.9235\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1674\t Accuracy 0.9200\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1674\t Accuracy 0.9217\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1673\t Accuracy 0.9217\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1674\t Accuracy 0.9216\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1676\t Accuracy 0.9206\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1676\t Accuracy 0.9206\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1677\t Accuracy 0.9208\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1677\t Accuracy 0.9198\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1677\t Average training accuracy 0.9199\n",
      "Epoch [11]\t Average validation loss 0.1644\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1645\t Accuracy 0.9400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1662\t Accuracy 0.9269\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1664\t Accuracy 0.9238\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1670\t Accuracy 0.9205\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1670\t Accuracy 0.9224\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1670\t Accuracy 0.9224\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1670\t Accuracy 0.9223\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1672\t Accuracy 0.9213\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1672\t Accuracy 0.9214\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1673\t Accuracy 0.9216\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1673\t Accuracy 0.9207\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1673\t Average training accuracy 0.9209\n",
      "Epoch [12]\t Average validation loss 0.1641\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1642\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1658\t Accuracy 0.9282\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1661\t Accuracy 0.9250\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1666\t Accuracy 0.9217\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1667\t Accuracy 0.9234\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1666\t Accuracy 0.9235\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1667\t Accuracy 0.9234\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1669\t Accuracy 0.9225\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1669\t Accuracy 0.9225\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1669\t Accuracy 0.9228\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1670\t Accuracy 0.9217\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1670\t Average training accuracy 0.9218\n",
      "Epoch [13]\t Average validation loss 0.1638\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1639\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1656\t Accuracy 0.9292\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1658\t Accuracy 0.9261\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1663\t Accuracy 0.9229\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1664\t Accuracy 0.9244\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1663\t Accuracy 0.9247\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1664\t Accuracy 0.9246\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1666\t Accuracy 0.9236\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1666\t Accuracy 0.9235\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1667\t Accuracy 0.9237\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1667\t Accuracy 0.9227\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1667\t Average training accuracy 0.9227\n",
      "Epoch [14]\t Average validation loss 0.1636\t Average validation accuracy 0.9436\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1636\t Accuracy 0.9400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1653\t Accuracy 0.9294\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1656\t Accuracy 0.9264\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1661\t Accuracy 0.9232\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1661\t Accuracy 0.9248\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1661\t Accuracy 0.9251\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1661\t Accuracy 0.9250\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1663\t Accuracy 0.9241\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1664\t Accuracy 0.9241\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1664\t Accuracy 0.9242\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1665\t Accuracy 0.9233\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1665\t Average training accuracy 0.9234\n",
      "Epoch [15]\t Average validation loss 0.1634\t Average validation accuracy 0.9440\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1634\t Accuracy 0.9400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1651\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1653\t Accuracy 0.9271\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1659\t Accuracy 0.9240\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1659\t Accuracy 0.9256\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1658\t Accuracy 0.9260\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1659\t Accuracy 0.9257\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1661\t Accuracy 0.9248\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1661\t Accuracy 0.9248\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1662\t Accuracy 0.9249\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1663\t Accuracy 0.9239\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1662\t Average training accuracy 0.9240\n",
      "Epoch [16]\t Average validation loss 0.1632\t Average validation accuracy 0.9446\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1632\t Accuracy 0.9400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1649\t Accuracy 0.9306\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1651\t Accuracy 0.9277\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1657\t Accuracy 0.9247\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1657\t Accuracy 0.9264\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1656\t Accuracy 0.9267\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1657\t Accuracy 0.9264\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1659\t Accuracy 0.9256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1659\t Accuracy 0.9256\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1660\t Accuracy 0.9256\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1661\t Accuracy 0.9247\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1660\t Average training accuracy 0.9248\n",
      "Epoch [17]\t Average validation loss 0.1630\t Average validation accuracy 0.9448\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1630\t Accuracy 0.9400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1647\t Accuracy 0.9314\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1650\t Accuracy 0.9283\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1655\t Accuracy 0.9251\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1655\t Accuracy 0.9269\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1655\t Accuracy 0.9273\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1655\t Accuracy 0.9269\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1657\t Accuracy 0.9261\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1658\t Accuracy 0.9260\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1658\t Accuracy 0.9261\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1659\t Accuracy 0.9252\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1659\t Average training accuracy 0.9253\n",
      "Epoch [18]\t Average validation loss 0.1629\t Average validation accuracy 0.9452\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1628\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1645\t Accuracy 0.9322\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1648\t Accuracy 0.9286\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1653\t Accuracy 0.9256\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1653\t Accuracy 0.9274\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1653\t Accuracy 0.9275\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1654\t Accuracy 0.9272\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1656\t Accuracy 0.9264\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1656\t Accuracy 0.9264\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1656\t Accuracy 0.9265\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1657\t Accuracy 0.9257\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1657\t Average training accuracy 0.9258\n",
      "Epoch [19]\t Average validation loss 0.1627\t Average validation accuracy 0.9466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9310.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2453\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2130\t Accuracy 0.4406\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2031\t Accuracy 0.5812\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1972\t Accuracy 0.6492\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1928\t Accuracy 0.6969\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1895\t Accuracy 0.7275\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1870\t Accuracy 0.7501\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1851\t Accuracy 0.7655\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.1834\t Accuracy 0.7789\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.1820\t Accuracy 0.7898\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.1809\t Accuracy 0.7984\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1798\t Average training accuracy 0.8067\n",
      "Epoch [0]\t Average validation loss 0.1640\t Average validation accuracy 0.9226\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1629\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1667\t Accuracy 0.9108\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1668\t Accuracy 0.9063\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1672\t Accuracy 0.9030\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1670\t Accuracy 0.9047\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1669\t Accuracy 0.9051\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1669\t Accuracy 0.9051\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1669\t Accuracy 0.9043\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1668\t Accuracy 0.9043\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1667\t Accuracy 0.9043\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1667\t Accuracy 0.9040\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1666\t Average training accuracy 0.9048\n",
      "Epoch [1]\t Average validation loss 0.1610\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1596\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1635\t Accuracy 0.9255\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1637\t Accuracy 0.9218\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1643\t Accuracy 0.9189\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1642\t Accuracy 0.9203\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1642\t Accuracy 0.9208\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1643\t Accuracy 0.9208\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1644\t Accuracy 0.9201\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1643\t Accuracy 0.9198\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1644\t Accuracy 0.9198\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1644\t Accuracy 0.9191\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1643\t Average training accuracy 0.9194\n",
      "Epoch [2]\t Average validation loss 0.1598\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1583\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1621\t Accuracy 0.9325\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1624\t Accuracy 0.9294\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1629\t Accuracy 0.9267\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1628\t Accuracy 0.9279\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1628\t Accuracy 0.9283\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1629\t Accuracy 0.9282\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1631\t Accuracy 0.9275\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1631\t Accuracy 0.9271\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1631\t Accuracy 0.9273\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1632\t Accuracy 0.9264\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1631\t Average training accuracy 0.9268\n",
      "Epoch [3]\t Average validation loss 0.1591\t Average validation accuracy 0.9518\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1576\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1612\t Accuracy 0.9380\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1615\t Accuracy 0.9350\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1620\t Accuracy 0.9327\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1619\t Accuracy 0.9334\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1620\t Accuracy 0.9342\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1621\t Accuracy 0.9342\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1622\t Accuracy 0.9332\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1622\t Accuracy 0.9331\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1622\t Accuracy 0.9332\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1623\t Accuracy 0.9322\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1623\t Average training accuracy 0.9325\n",
      "Epoch [4]\t Average validation loss 0.1587\t Average validation accuracy 0.9542\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1571\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1605\t Accuracy 0.9424\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1609\t Accuracy 0.9389\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1613\t Accuracy 0.9370\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1613\t Accuracy 0.9377\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1613\t Accuracy 0.9384\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1614\t Accuracy 0.9382\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1615\t Accuracy 0.9374\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1616\t Accuracy 0.9371\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1616\t Accuracy 0.9372\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1617\t Accuracy 0.9362\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1616\t Average training accuracy 0.9364\n",
      "Epoch [5]\t Average validation loss 0.1583\t Average validation accuracy 0.9572\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1567\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1600\t Accuracy 0.9457\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1604\t Accuracy 0.9421\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1608\t Accuracy 0.9403\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1608\t Accuracy 0.9407\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1608\t Accuracy 0.9416\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1609\t Accuracy 0.9416\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1610\t Accuracy 0.9409\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1610\t Accuracy 0.9407\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1610\t Accuracy 0.9408\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1611\t Accuracy 0.9398\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1611\t Average training accuracy 0.9401\n",
      "Epoch [6]\t Average validation loss 0.1580\t Average validation accuracy 0.9588\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1564\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1596\t Accuracy 0.9476\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1600\t Accuracy 0.9454\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1604\t Accuracy 0.9434\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1603\t Accuracy 0.9439\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1604\t Accuracy 0.9444\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1605\t Accuracy 0.9444\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1606\t Accuracy 0.9438\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1606\t Accuracy 0.9439\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1606\t Accuracy 0.9441\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1607\t Accuracy 0.9430\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1607\t Average training accuracy 0.9432\n",
      "Epoch [7]\t Average validation loss 0.1577\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1562\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1593\t Accuracy 0.9508\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1596\t Accuracy 0.9480\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1601\t Accuracy 0.9463\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1600\t Accuracy 0.9468\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1600\t Accuracy 0.9472\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1601\t Accuracy 0.9471\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1602\t Accuracy 0.9467\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1602\t Accuracy 0.9467\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1602\t Accuracy 0.9470\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1603\t Accuracy 0.9460\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1603\t Average training accuracy 0.9461\n",
      "Epoch [8]\t Average validation loss 0.1573\t Average validation accuracy 0.9632\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1559\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1590\t Accuracy 0.9522\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1593\t Accuracy 0.9495\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1597\t Accuracy 0.9482\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1596\t Accuracy 0.9485\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1596\t Accuracy 0.9490\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1598\t Accuracy 0.9490\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1599\t Accuracy 0.9484\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1599\t Accuracy 0.9484\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1599\t Accuracy 0.9486\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1600\t Accuracy 0.9477\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1600\t Average training accuracy 0.9478\n",
      "Epoch [9]\t Average validation loss 0.1571\t Average validation accuracy 0.9640\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1559\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1587\t Accuracy 0.9553\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1590\t Accuracy 0.9520\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1594\t Accuracy 0.9509\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1593\t Accuracy 0.9510\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1593\t Accuracy 0.9515\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1595\t Accuracy 0.9512\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1596\t Accuracy 0.9507\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1596\t Accuracy 0.9506\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1596\t Accuracy 0.9506\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1597\t Accuracy 0.9498\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1597\t Average training accuracy 0.9499\n",
      "Epoch [10]\t Average validation loss 0.1570\t Average validation accuracy 0.9648\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1559\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1584\t Accuracy 0.9569\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1588\t Accuracy 0.9540\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1591\t Accuracy 0.9527\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1591\t Accuracy 0.9530\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1591\t Accuracy 0.9535\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1592\t Accuracy 0.9529\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1593\t Accuracy 0.9522\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1593\t Accuracy 0.9520\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1593\t Accuracy 0.9521\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1594\t Accuracy 0.9513\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1594\t Average training accuracy 0.9514\n",
      "Epoch [11]\t Average validation loss 0.1568\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1557\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1582\t Accuracy 0.9580\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1585\t Accuracy 0.9552\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1589\t Accuracy 0.9540\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1588\t Accuracy 0.9543\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1588\t Accuracy 0.9549\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1590\t Accuracy 0.9542\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1590\t Accuracy 0.9537\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1591\t Accuracy 0.9536\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1591\t Accuracy 0.9535\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1592\t Accuracy 0.9528\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1592\t Average training accuracy 0.9529\n",
      "Epoch [12]\t Average validation loss 0.1565\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1556\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1579\t Accuracy 0.9592\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1583\t Accuracy 0.9564\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1587\t Accuracy 0.9556\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1586\t Accuracy 0.9557\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1586\t Accuracy 0.9562\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1588\t Accuracy 0.9555\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1588\t Accuracy 0.9551\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1589\t Accuracy 0.9548\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1589\t Accuracy 0.9548\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1590\t Accuracy 0.9541\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1590\t Average training accuracy 0.9543\n",
      "Epoch [13]\t Average validation loss 0.1564\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1554\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1578\t Accuracy 0.9606\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1581\t Accuracy 0.9584\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1585\t Accuracy 0.9571\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1584\t Accuracy 0.9575\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1584\t Accuracy 0.9577\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1586\t Accuracy 0.9571\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1586\t Accuracy 0.9567\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1587\t Accuracy 0.9565\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1587\t Accuracy 0.9567\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1588\t Accuracy 0.9559\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1587\t Average training accuracy 0.9560\n",
      "Epoch [14]\t Average validation loss 0.1563\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1553\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1576\t Accuracy 0.9616\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1580\t Accuracy 0.9597\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1583\t Accuracy 0.9583\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1583\t Accuracy 0.9586\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1583\t Accuracy 0.9587\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1584\t Accuracy 0.9580\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1585\t Accuracy 0.9576\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1585\t Accuracy 0.9575\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1585\t Accuracy 0.9576\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1586\t Accuracy 0.9569\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1586\t Average training accuracy 0.9570\n",
      "Epoch [15]\t Average validation loss 0.1561\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1551\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1574\t Accuracy 0.9629\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1578\t Accuracy 0.9611\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1581\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1581\t Accuracy 0.9602\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1581\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1582\t Accuracy 0.9591\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1583\t Accuracy 0.9586\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1583\t Accuracy 0.9584\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1583\t Accuracy 0.9585\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1585\t Accuracy 0.9578\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1584\t Average training accuracy 0.9579\n",
      "Epoch [16]\t Average validation loss 0.1561\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1550\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1573\t Accuracy 0.9635\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1577\t Accuracy 0.9616\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1580\t Accuracy 0.9605\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1580\t Accuracy 0.9608\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1580\t Accuracy 0.9608\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1581\t Accuracy 0.9599\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1582\t Accuracy 0.9594\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1582\t Accuracy 0.9591\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1582\t Accuracy 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1583\t Accuracy 0.9585\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1583\t Average training accuracy 0.9587\n",
      "Epoch [17]\t Average validation loss 0.1559\t Average validation accuracy 0.9682\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1548\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1572\t Accuracy 0.9637\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1575\t Accuracy 0.9622\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1579\t Accuracy 0.9610\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1578\t Accuracy 0.9614\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1578\t Accuracy 0.9614\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1580\t Accuracy 0.9606\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1580\t Accuracy 0.9603\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1581\t Accuracy 0.9599\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1581\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1582\t Accuracy 0.9593\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1582\t Average training accuracy 0.9594\n",
      "Epoch [18]\t Average validation loss 0.1558\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1546\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1571\t Accuracy 0.9639\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1574\t Accuracy 0.9625\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1577\t Accuracy 0.9618\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1577\t Accuracy 0.9621\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1577\t Accuracy 0.9622\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1578\t Accuracy 0.9614\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1579\t Accuracy 0.9609\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1579\t Accuracy 0.9607\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1579\t Accuracy 0.9608\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1581\t Accuracy 0.9601\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1580\t Average training accuracy 0.9602\n",
      "Epoch [19]\t Average validation loss 0.1557\t Average validation accuracy 0.9700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9593.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework3, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\_YiyuanData\\大四\\课程\\深度学习导论\\homework-3\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2564\t Accuracy 0.0500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2162\t Accuracy 0.4163\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2063\t Accuracy 0.5524\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.2007\t Accuracy 0.6244\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1963\t Accuracy 0.6764\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1927\t Accuracy 0.7136\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1899\t Accuracy 0.7398\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1877\t Accuracy 0.7593\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.1857\t Accuracy 0.7761\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.1840\t Accuracy 0.7897\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.1826\t Accuracy 0.8001\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1813\t Average training accuracy 0.8099\n",
      "Epoch [0]\t Average validation loss 0.1650\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1644\t Accuracy 0.9100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1662\t Accuracy 0.9212\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1661\t Accuracy 0.9186\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1663\t Accuracy 0.9173\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1661\t Accuracy 0.9186\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1658\t Accuracy 0.9203\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1656\t Accuracy 0.9209\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1655\t Accuracy 0.9217\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1652\t Accuracy 0.9231\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1650\t Accuracy 0.9239\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1649\t Accuracy 0.9242\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1647\t Average training accuracy 0.9254\n",
      "Epoch [1]\t Average validation loss 0.1598\t Average validation accuracy 0.9530\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1594\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1612\t Accuracy 0.9420\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1614\t Accuracy 0.9388\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1617\t Accuracy 0.9377\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1617\t Accuracy 0.9390\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1616\t Accuracy 0.9397\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1615\t Accuracy 0.9395\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1615\t Accuracy 0.9399\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1614\t Accuracy 0.9406\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1613\t Accuracy 0.9409\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1613\t Accuracy 0.9407\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1612\t Average training accuracy 0.9412\n",
      "Epoch [2]\t Average validation loss 0.1576\t Average validation accuracy 0.9614\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1576\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1591\t Accuracy 0.9527\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1593\t Accuracy 0.9505\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1596\t Accuracy 0.9497\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1596\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1595\t Accuracy 0.9503\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1595\t Accuracy 0.9497\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1595\t Accuracy 0.9498\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1595\t Accuracy 0.9502\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1594\t Accuracy 0.9505\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1595\t Accuracy 0.9502\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1594\t Average training accuracy 0.9505\n",
      "Epoch [3]\t Average validation loss 0.1563\t Average validation accuracy 0.9646\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1565\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1577\t Accuracy 0.9588\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1580\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1583\t Accuracy 0.9553\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1582\t Accuracy 0.9560\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1581\t Accuracy 0.9561\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1582\t Accuracy 0.9559\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1582\t Accuracy 0.9558\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1582\t Accuracy 0.9562\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1581\t Accuracy 0.9566\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1582\t Accuracy 0.9560\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1582\t Average training accuracy 0.9563\n",
      "Epoch [4]\t Average validation loss 0.1554\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1556\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1567\t Accuracy 0.9649\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1570\t Accuracy 0.9617\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1573\t Accuracy 0.9615\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1572\t Accuracy 0.9617\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1572\t Accuracy 0.9615\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1572\t Accuracy 0.9610\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1572\t Accuracy 0.9609\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1572\t Accuracy 0.9613\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1572\t Accuracy 0.9613\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1573\t Accuracy 0.9607\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1572\t Average training accuracy 0.9609\n",
      "Epoch [5]\t Average validation loss 0.1548\t Average validation accuracy 0.9710\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1550\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1559\t Accuracy 0.9686\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1563\t Accuracy 0.9652\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1565\t Accuracy 0.9650\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1565\t Accuracy 0.9650\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1564\t Accuracy 0.9648\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1565\t Accuracy 0.9643\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1565\t Accuracy 0.9644\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1565\t Accuracy 0.9648\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1565\t Accuracy 0.9647\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1566\t Accuracy 0.9641\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1565\t Average training accuracy 0.9641\n",
      "Epoch [6]\t Average validation loss 0.1544\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1545\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1553\t Accuracy 0.9712\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1557\t Accuracy 0.9682\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1559\t Accuracy 0.9680\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1558\t Accuracy 0.9680\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1558\t Accuracy 0.9677\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1559\t Accuracy 0.9673\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1559\t Accuracy 0.9672\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1559\t Accuracy 0.9676\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1559\t Accuracy 0.9675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1560\t Accuracy 0.9668\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1559\t Average training accuracy 0.9669\n",
      "Epoch [7]\t Average validation loss 0.1540\t Average validation accuracy 0.9754\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1541\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1549\t Accuracy 0.9724\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1552\t Accuracy 0.9698\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1554\t Accuracy 0.9703\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1553\t Accuracy 0.9704\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1553\t Accuracy 0.9702\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1554\t Accuracy 0.9699\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1554\t Accuracy 0.9699\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1554\t Accuracy 0.9701\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1554\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1555\t Accuracy 0.9693\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1555\t Average training accuracy 0.9694\n",
      "Epoch [8]\t Average validation loss 0.1536\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1537\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1545\t Accuracy 0.9741\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1548\t Accuracy 0.9718\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1550\t Accuracy 0.9723\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1549\t Accuracy 0.9722\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1549\t Accuracy 0.9723\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1550\t Accuracy 0.9719\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1550\t Accuracy 0.9719\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1550\t Accuracy 0.9720\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1550\t Accuracy 0.9721\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1551\t Accuracy 0.9714\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1551\t Average training accuracy 0.9715\n",
      "Epoch [9]\t Average validation loss 0.1533\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1533\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1542\t Accuracy 0.9751\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1544\t Accuracy 0.9734\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1546\t Accuracy 0.9743\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1546\t Accuracy 0.9743\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1545\t Accuracy 0.9744\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1546\t Accuracy 0.9740\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1546\t Accuracy 0.9739\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1547\t Accuracy 0.9739\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1547\t Accuracy 0.9739\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1548\t Accuracy 0.9731\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1547\t Average training accuracy 0.9732\n",
      "Epoch [10]\t Average validation loss 0.1531\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1530\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1539\t Accuracy 0.9763\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1541\t Accuracy 0.9748\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1543\t Accuracy 0.9758\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1543\t Accuracy 0.9755\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1542\t Accuracy 0.9757\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1543\t Accuracy 0.9752\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1543\t Accuracy 0.9752\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1544\t Accuracy 0.9752\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1544\t Accuracy 0.9751\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1545\t Accuracy 0.9745\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1545\t Average training accuracy 0.9745\n",
      "Epoch [11]\t Average validation loss 0.1529\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1529\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1537\t Accuracy 0.9773\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1539\t Accuracy 0.9757\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1540\t Accuracy 0.9769\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1540\t Accuracy 0.9769\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1540\t Accuracy 0.9770\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1541\t Accuracy 0.9765\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1541\t Accuracy 0.9766\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1541\t Accuracy 0.9767\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1541\t Accuracy 0.9764\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1542\t Accuracy 0.9758\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1542\t Average training accuracy 0.9759\n",
      "Epoch [12]\t Average validation loss 0.1528\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1528\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1534\t Accuracy 0.9775\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1537\t Accuracy 0.9764\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1538\t Accuracy 0.9777\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1538\t Accuracy 0.9778\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1538\t Accuracy 0.9778\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1539\t Accuracy 0.9775\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1539\t Accuracy 0.9775\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1539\t Accuracy 0.9776\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1539\t Accuracy 0.9773\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1540\t Accuracy 0.9768\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1540\t Average training accuracy 0.9768\n",
      "Epoch [13]\t Average validation loss 0.1527\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1527\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1533\t Accuracy 0.9790\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1535\t Accuracy 0.9777\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1536\t Accuracy 0.9787\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1536\t Accuracy 0.9789\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1536\t Accuracy 0.9789\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1537\t Accuracy 0.9786\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1537\t Accuracy 0.9785\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1537\t Accuracy 0.9785\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1537\t Accuracy 0.9782\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1538\t Accuracy 0.9777\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1538\t Average training accuracy 0.9778\n",
      "Epoch [14]\t Average validation loss 0.1526\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1526\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1531\t Accuracy 0.9798\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1533\t Accuracy 0.9784\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1534\t Accuracy 0.9797\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1535\t Accuracy 0.9800\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1534\t Accuracy 0.9799\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1535\t Accuracy 0.9797\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1535\t Accuracy 0.9796\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1535\t Accuracy 0.9795\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1535\t Accuracy 0.9792\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1536\t Accuracy 0.9786\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1536\t Average training accuracy 0.9787\n",
      "Epoch [15]\t Average validation loss 0.1525\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1525\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1529\t Accuracy 0.9802\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1532\t Accuracy 0.9793\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1533\t Accuracy 0.9803\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1533\t Accuracy 0.9807\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1533\t Accuracy 0.9807\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1534\t Accuracy 0.9805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1534\t Accuracy 0.9805\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1534\t Accuracy 0.9806\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1534\t Accuracy 0.9802\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1535\t Accuracy 0.9797\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1535\t Average training accuracy 0.9797\n",
      "Epoch [16]\t Average validation loss 0.1524\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1523\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1528\t Accuracy 0.9814\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1530\t Accuracy 0.9801\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1531\t Accuracy 0.9811\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1532\t Accuracy 0.9815\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1531\t Accuracy 0.9817\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1532\t Accuracy 0.9816\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1532\t Accuracy 0.9815\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1532\t Accuracy 0.9816\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1533\t Accuracy 0.9812\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1533\t Accuracy 0.9808\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1533\t Average training accuracy 0.9808\n",
      "Epoch [17]\t Average validation loss 0.1523\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1523\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1527\t Accuracy 0.9818\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1529\t Accuracy 0.9811\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1530\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1530\t Accuracy 0.9822\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1530\t Accuracy 0.9824\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1531\t Accuracy 0.9823\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1531\t Accuracy 0.9822\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1531\t Accuracy 0.9824\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1531\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1532\t Accuracy 0.9815\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1532\t Average training accuracy 0.9815\n",
      "Epoch [18]\t Average validation loss 0.1522\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1522\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1526\t Accuracy 0.9824\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1528\t Accuracy 0.9817\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1529\t Accuracy 0.9824\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1529\t Accuracy 0.9827\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1529\t Accuracy 0.9829\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1530\t Accuracy 0.9828\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1530\t Accuracy 0.9827\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1530\t Accuracy 0.9829\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1530\t Accuracy 0.9825\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1531\t Accuracy 0.9821\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1531\t Average training accuracy 0.9821\n",
      "Epoch [19]\t Average validation loss 0.1522\t Average validation accuracy 0.9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9753.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d50892c7193ec8d92aba2866d7fd9025d172bc37f5538f4292389db9e9b4aec7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
